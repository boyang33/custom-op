#!/usr/bin/env python3
"""
TensorConcatWithOffsets + TensorSplitByOffsets è”åˆæµ‹è¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬æµ‹è¯•ä¸¤ä¸ªè‡ªå®šä¹‰ç®—å­çš„è”åˆä½¿ç”¨ï¼š
1. tensor_concat_with_offsets: å°†å¤šä¸ªtensoråˆå¹¶ä¸ºä¸€ä¸ªï¼Œå¹¶ç”Ÿæˆåç§»é‡ä¿¡æ¯
2. tensor_split_by_offsets: æ ¹æ®åç§»é‡ä¿¡æ¯å°†åˆå¹¶çš„tensoræ‹†åˆ†å›åŸå§‹tensoråˆ—è¡¨

æµ‹è¯•å†…å®¹åŒ…æ‹¬ï¼š
- æ­£ç¡®æ€§éªŒè¯ï¼šå¾€è¿”æµ‹è¯•ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
- æ€§èƒ½å¯¹æ¯”ï¼šä¸æ ‡å‡†TensorFlowç®—å­çš„æ€§èƒ½å¯¹æ¯”
- å‚æ•°è°ƒä¼˜ï¼šä¸åŒalignmentå’Œé…ç½®ä¸‹çš„æ€§èƒ½æµ‹è¯•
- è¾¹ç•Œæ¡ä»¶ï¼šå„ç§ç‰¹æ®Šæƒ…å†µçš„å¤„ç†
"""

import tensorflow as tf
import numpy as np
import time
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥è‡ªå®šä¹‰ç®—å­
from tensor_concat_with_offsets.python.ops import tensor_concat_with_offsets_ops
from tensor_split_by_offsets.python.ops import tensor_split_by_offsets_ops


class ConcatSplitTester:
    """TensorConcatWithOffsets + TensorSplitByOffsets è”åˆæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = []
        
    def log_result(self, test_name, passed, details=""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        status = "âœ… PASS" if passed else "âŒ FAIL"
        result = {
            'test_name': test_name,
            'passed': passed,
            'details': details,
            'status': status
        }
        self.test_results.append(result)
        print(f"{status}: {test_name}")
        if details:
            print(f"   {details}")
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        total = len(self.test_results)
        passed = sum(1 for r in self.test_results if r['passed'])
        failed = total - passed
        
        print("\n" + "="*80)
        print("æµ‹è¯•æ€»ç»“")
        print("="*80)
        print(f"æ€»æµ‹è¯•æ•°: {total}")
        print(f"é€šè¿‡: {passed}")
        print(f"å¤±è´¥: {failed}")
        print(f"é€šè¿‡ç‡: {passed/total*100:.1f}%")
        
        if failed > 0:
            print("\nå¤±è´¥çš„æµ‹è¯•:")
            for result in self.test_results:
                if not result['passed']:
                    print(f"  - {result['test_name']}: {result['details']}")

    def test_basic_roundtrip(self):
        """åŸºç¡€å¾€è¿”æµ‹è¯•"""
        print("\n=== åŸºç¡€å¾€è¿”æµ‹è¯• ===")
        
        # æµ‹è¯•æ•°æ®
        test_cases = [
            # 1D tensors
            {
                'name': '1Dæ•´å‹tensor',
                'tensors': [
                    tf.constant([1, 2, 3], dtype=tf.int32),
                    tf.constant([4, 5], dtype=tf.int32),
                    tf.constant([6, 7, 8, 9], dtype=tf.int32)
                ]
            },
            # 2D tensors
            {
                'name': '2Dæµ®ç‚¹tensor',
                'tensors': [
                    tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32),
                    tf.constant([[5.0, 6.0]], dtype=tf.float32),
                    tf.constant([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], dtype=tf.float32)
                ]
            },
            # 3D tensors
            {
                'name': '3D tensor',
                'tensors': [
                    tf.constant([[[1, 2]], [[3, 4]]], dtype=tf.int32),
                    tf.constant([[[5, 6]]], dtype=tf.int32),
                    tf.constant([[[7, 8]], [[9, 10]], [[11, 12]]], dtype=tf.int32)
                ]
            }
        ]
        
        for case in test_cases:
            try:
                original_tensors = case['tensors']
                
                # æ­¥éª¤1: ä½¿ç”¨concatç®—å­åˆå¹¶
                merged_tensor, offsets = tensor_concat_with_offsets_ops.tensor_concat_with_offsets(
                    original_tensors, alignment=64
                )
                
                # æ­¥éª¤2: ä½¿ç”¨splitç®—å­æ‹†åˆ†
                restored_tensors = tensor_split_by_offsets_ops.tensor_split_by_offsets(
                    merged_tensor, offsets
                )
                
                # æ­¥éª¤3: éªŒè¯æ•°æ®ä¸€è‡´æ€§
                all_match = True
                max_error = 0.0
                
                for i, (original, restored) in enumerate(zip(original_tensors, restored_tensors)):
                    if original.dtype.is_floating:
                        error = tf.reduce_max(tf.abs(original - restored)).numpy()
                        max_error = max(max_error, error)
                        if error > 1e-6:
                            all_match = False
                            break
                    else:
                        if not tf.reduce_all(tf.equal(original, restored)).numpy():
                            all_match = False
                            break
                
                details = f"æœ€å¤§è¯¯å·®: {max_error:.2e}" if original_tensors[0].dtype.is_floating else "å®Œå…¨åŒ¹é…"
                self.log_result(f"å¾€è¿”æµ‹è¯•_{case['name']}", all_match, details)
                
            except Exception as e:
                self.log_result(f"å¾€è¿”æµ‹è¯•_{case['name']}", False, f"å¼‚å¸¸: {str(e)}")

    def test_alignment_parameters(self):
        """æµ‹è¯•ä¸åŒalignmentå‚æ•°çš„å½±å“"""
        print("\n=== Alignmentå‚æ•°æµ‹è¯• ===")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_tensors = [
            tf.constant(np.random.rand(100, 32).astype(np.float32)),
            tf.constant(np.random.rand(50, 32).astype(np.float32)),
            tf.constant(np.random.rand(200, 32).astype(np.float32))
        ]
        
        alignments = [16, 32, 64, 128, 256]
        
        for alignment in alignments:
            try:
                # åˆå¹¶
                merged, offsets = tensor_concat_with_offsets_ops.tensor_concat_with_offsets(
                    test_tensors, alignment=alignment
                )
                
                # æ‹†åˆ†
                restored = tensor_split_by_offsets_ops.tensor_split_by_offsets(
                    merged, offsets
                )
                
                # éªŒè¯æ­£ç¡®æ€§
                all_correct = True
                for orig, rest in zip(test_tensors, restored):
                    if not np.allclose(orig.numpy(), rest.numpy(), rtol=1e-6):
                        all_correct = False
                        break
                
                # è®¡ç®—å†…å­˜å¼€é”€
                original_size = sum(t.shape[0] for t in test_tensors)
                aligned_size = merged.shape[0]
                overhead = (aligned_size - original_size) / original_size * 100
                
                details = f"å†…å­˜å¼€é”€: {overhead:.1f}%"
                self.log_result(f"Alignment_{alignment}", all_correct, details)
                
            except Exception as e:
                self.log_result(f"Alignment_{alignment}", False, f"å¼‚å¸¸: {str(e)}")

    def test_zero_copy_modes(self):
        """æµ‹è¯•é›¶æ‹·è´æ¨¡å¼çš„å½±å“"""
        print("\n=== é›¶æ‹·è´æ¨¡å¼æµ‹è¯• ===")
        
        # åˆ›å»ºå¯¹é½çš„æµ‹è¯•æ•°æ®
        test_tensors = [
            tf.constant(np.random.rand(1000, 64).astype(np.float32)),
            tf.constant(np.random.rand(500, 64).astype(np.float32)),
            tf.constant(np.random.rand(1500, 64).astype(np.float32))
        ]
        
        # ä½¿ç”¨å¯¹é½åˆå¹¶ç¡®ä¿é›¶æ‹·è´æ¡ä»¶
        merged, offsets = tensor_concat_with_offsets_ops.tensor_concat_with_offsets(
            test_tensors, alignment=64
        )
        
        alignment_modes = [True, False]
        
        for use_alignment in alignment_modes:
            try:
                # æµ‹é‡æ‹†åˆ†æ€§èƒ½
                start_time = time.time()
                restored = tensor_split_by_offsets_ops.tensor_split_by_offsets(
                    merged, offsets, use_alignment=use_alignment
                )
                # å¼ºåˆ¶æ‰§è¡Œ
                _ = [t.numpy() for t in restored]
                execution_time = time.time() - start_time
                
                # éªŒè¯æ­£ç¡®æ€§
                all_correct = True
                for orig, rest in zip(test_tensors, restored):
                    if not np.allclose(orig.numpy(), rest.numpy(), rtol=1e-6):
                        all_correct = False
                        break
                
                mode_name = "å¯¹é½ä¼˜åŒ–" if use_alignment else "æ•°æ®å¤åˆ¶"
                details = f"æ‰§è¡Œæ—¶é—´: {execution_time*1000:.2f}ms"
                self.log_result(f"æ‹†åˆ†æ¨¡å¼_{mode_name}", all_correct, details)
                
            except Exception as e:
                mode_name = "å¯¹é½ä¼˜åŒ–" if use_alignment else "æ•°æ®å¤åˆ¶"
                self.log_result(f"æ‹†åˆ†æ¨¡å¼_{mode_name}", False, f"å¼‚å¸¸: {str(e)}")

    def test_performance_comparison(self):
        """æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼šå››ç§å…¸å‹åœºæ™¯"""
        print("\n=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")
        
        rng = np.random.default_rng(42)
        benchmark_tensors = [
            tf.constant(rng.standard_normal((5000, 128)).astype(np.float32)),
            tf.constant(rng.standard_normal((3000, 128)).astype(np.float32)),
            tf.constant(rng.standard_normal((8000, 128)).astype(np.float32)),
            tf.constant(rng.standard_normal((2000, 128)).astype(np.float32))
        ]
        lengths = [int(tensor.shape[0]) for tensor in benchmark_tensors]
        warmup_iterations = 3
        test_iterations = 10
        
        def materialize(value):
            if isinstance(value, (list, tuple)):
                return [tensor.numpy() for tensor in value]
            return value.numpy()
        
        def validate(restored_tensors):
            if len(restored_tensors) != len(benchmark_tensors):
                return False
            for original, candidate in zip(benchmark_tensors, restored_tensors):
                if not np.allclose(original.numpy(), candidate.numpy(), rtol=1e-6, atol=1e-6):
                    return False
            return True
        
        def run_concat_with_offsets_and_split():
            merged, offsets = tensor_concat_with_offsets_ops.tensor_concat_with_offsets(
                benchmark_tensors, alignment=64, use_alignment=True)
            restored = tensor_split_by_offsets_ops.tensor_split_by_offsets(
                merged, offsets, alignment=64, use_alignment=True)
            return merged, restored
        
        def run_concat_with_offsets_and_slice():
            merged, offsets = tensor_concat_with_offsets_ops.tensor_concat_with_offsets(
                benchmark_tensors, alignment=64, use_alignment=True)
            offsets_array = offsets.numpy()
            slices = []
            rank = merged.shape.rank
            if rank is None:
                rank = int(tf.rank(merged).numpy())
            for start, length in offsets_array:
                start = int(start)
                length = int(length)
                begin = [start] + [0] * (rank - 1)
                size = [length] + [-1] * (rank - 1)
                slices.append(tf.slice(merged, begin, size))
            return merged, slices
        
        def run_tf_concat_split():
            merged = tf.concat(benchmark_tensors, axis=0)
            restored = tf.split(merged, lengths, axis=0)
            return merged, restored
        
        scenario_configs = [
            ("tensor_concat_with_offsets + tensor_split_by_offsets", run_concat_with_offsets_and_split),
            ("tensor_concat_with_offsets + tf.slice", run_concat_with_offsets_and_slice),
            ("tf.concat + tf.split", run_tf_concat_split),
        ]
        
        scenario_results = []
        
        for name, runner in scenario_configs:
            try:
                for _ in range(warmup_iterations):
                    merged, restored = runner()
                    materialize(merged)
                    materialize(restored)
                
                times = []
                final_restored = None
                for _ in range(test_iterations):
                    start_time = time.time()
                    merged, restored = runner()
                    materialize(merged)
                    materialize(restored)
                    times.append(time.time() - start_time)
                    final_restored = restored
                
                avg_ms = np.mean(times) * 1000.0
                std_ms = np.std(times) * 1000.0
                passed = validate(final_restored)
                scenario_results.append({
                    "name": name,
                    "avg_ms": avg_ms,
                    "std_ms": std_ms,
                    "passed": passed
                })
            except Exception as exc:
                scenario_results.append({
                    "name": name,
                    "avg_ms": float("nan"),
                    "std_ms": float("nan"),
                    "passed": False,
                    "error": str(exc)
                })
        
        baseline = next((result for result in scenario_results if result["name"] == "tf.concat + tf.split"), None)
        
        print("\nåœºæ™¯æ€§èƒ½æ±‡æ€»:")
        for result in scenario_results:
            if not np.isfinite(result["avg_ms"]):
                print(f"- {result['name']}: æ‰§è¡Œå¤±è´¥ ({result.get('error', 'æœªçŸ¥é”™è¯¯')})")
                continue
            relative = ""
            if baseline and np.isfinite(baseline["avg_ms"]) and baseline["avg_ms"] > 0:
                if result["name"] == baseline["name"]:
                    relative = " (åŸºçº¿)"
                else:
                    ratio = baseline["avg_ms"] / result["avg_ms"]
                    relative = f" (ç›¸å¯¹tf.concat+tf.split: {ratio:.2f}x)"
            print(f"- {result['name']}: {result['avg_ms']:.2f} Â± {result['std_ms']:.2f} ms{relative}")
        
        for result in scenario_results:
            if not np.isfinite(result["avg_ms"]):
                self.log_result(f"æ€§èƒ½_{result['name']}", False, f"å¼‚å¸¸: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                continue
            details = f"å¹³å‡è€—æ—¶: {result['avg_ms']:.2f} Â± {result['std_ms']:.2f} ms"
            if baseline and np.isfinite(baseline["avg_ms"]) and baseline["avg_ms"] > 0:
                if result["name"] == baseline["name"]:
                    details += ", ä½œä¸ºåŸºçº¿"
                else:
                    ratio = baseline["avg_ms"] / result["avg_ms"]
                    details += f", ç›¸å¯¹åŸºçº¿æå‡: {ratio:.2f}x"
            self.log_result(f"æ€§èƒ½_{result['name']}", result["passed"], details)

    def test_edge_cases(self):
        """è¾¹ç•Œæ¡ä»¶æµ‹è¯•"""
        print("\n=== è¾¹ç•Œæ¡ä»¶æµ‹è¯• ===")
        
        edge_cases = [
            {
                'name': 'ç©ºtensor',
                'tensors': [
                    tf.constant([], dtype=tf.float32, shape=[0, 3]),
                    tf.constant([[1, 2, 3]], dtype=tf.float32),
                    tf.constant([], dtype=tf.float32, shape=[0, 3])
                ]
            },
            {
                'name': 'å•ä¸ªtensor',
                'tensors': [
                    tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)
                ]
            },
            {
                'name': 'å¤§é‡å°tensor',
                'tensors': [
                    tf.constant([[i]], dtype=tf.float32) for i in range(100)
                ]
            },
            {
                'name': 'å•è¡Œtensor',
                'tensors': [
                    tf.constant([[1, 2, 3, 4, 5]], dtype=tf.float32),
                    tf.constant([[6, 7, 8, 9, 10]], dtype=tf.float32)
                ]
            }
        ]
        
        for case in edge_cases:
            try:
                tensors = case['tensors']
                
                # åˆå¹¶
                merged, offsets = tensor_concat_with_offsets_ops.tensor_concat_with_offsets(
                    tensors, alignment=32
                )
                
                # æ‹†åˆ†
                restored = tensor_split_by_offsets_ops.tensor_split_by_offsets(
                    merged, offsets
                )
                
                # éªŒè¯
                all_correct = True
                for orig, rest in zip(tensors, restored):
                    if orig.shape[0] == 0:  # ç©ºtensorç‰¹æ®Šå¤„ç†
                        if rest.shape[0] != 0:
                            all_correct = False
                            break
                    else:
                        if not np.allclose(orig.numpy(), rest.numpy(), rtol=1e-6):
                            all_correct = False
                            break
                
                tensor_count = len(tensors)
                details = f"å¤„ç†äº† {tensor_count} ä¸ªtensor"
                self.log_result(f"è¾¹ç•Œæ¡ä»¶_{case['name']}", all_correct, details)
                
            except Exception as e:
                self.log_result(f"è¾¹ç•Œæ¡ä»¶_{case['name']}", False, f"å¼‚å¸¸: {str(e)}")

    def test_different_dtypes(self):
        """ä¸åŒæ•°æ®ç±»å‹æµ‹è¯•"""
        print("\n=== æ•°æ®ç±»å‹æµ‹è¯• ===")
        
        dtype_cases = [
            {
                'name': 'float32',
                'dtype': tf.float32,
                'data': [
                    [[1.1, 2.2], [3.3, 4.4]],
                    [[5.5, 6.6]],
                    [[7.7, 8.8], [9.9, 10.0]]
                ]
            },
            {
                'name': 'int32',
                'dtype': tf.int32,
                'data': [
                    [[1, 2], [3, 4]],
                    [[5, 6]],
                    [[7, 8], [9, 10]]
                ]
            },
            {
                'name': 'int64',
                'dtype': tf.int64,
                'data': [
                    [[1, 2], [3, 4]],
                    [[5, 6]],
                    [[7, 8], [9, 10]]
                ]
            }
        ]
        
        for case in dtype_cases:
            try:
                tensors = [tf.constant(data, dtype=case['dtype']) for data in case['data']]
                
                # åˆå¹¶
                merged, offsets = tensor_concat_with_offsets_ops.tensor_concat_with_offsets(
                    tensors, alignment=32
                )
                
                # æ‹†åˆ†
                restored = tensor_split_by_offsets_ops.tensor_split_by_offsets(
                    merged, offsets
                )
                
                # éªŒè¯
                all_correct = True
                for orig, rest in zip(tensors, restored):
                    if case['dtype'].is_floating:
                        if not np.allclose(orig.numpy(), rest.numpy(), rtol=1e-6):
                            all_correct = False
                            break
                    else:
                        if not tf.reduce_all(tf.equal(orig, rest)).numpy():
                            all_correct = False
                            break
                
                self.log_result(f"æ•°æ®ç±»å‹_{case['name']}", all_correct, f"dtype: {case['dtype']}")
                
            except Exception as e:
                self.log_result(f"æ•°æ®ç±»å‹_{case['name']}", False, f"å¼‚å¸¸: {str(e)}")

    def test_memory_usage(self):
        """å†…å­˜ä½¿ç”¨æµ‹è¯•"""
        print("\n=== å†…å­˜ä½¿ç”¨æµ‹è¯• ===")
        
        # åˆ›å»ºä¸åŒå¤§å°çš„æµ‹è¯•æ•°æ®
        size_cases = [
            {
                'name': 'å°æ•°æ®',
                'tensors': [
                    tf.constant(np.random.rand(10, 8).astype(np.float32)),
                    tf.constant(np.random.rand(5, 8).astype(np.float32)),
                    tf.constant(np.random.rand(15, 8).astype(np.float32))
                ]
            },
            {
                'name': 'ä¸­ç­‰æ•°æ®',
                'tensors': [
                    tf.constant(np.random.rand(1000, 64).astype(np.float32)),
                    tf.constant(np.random.rand(500, 64).astype(np.float32)),
                    tf.constant(np.random.rand(1500, 64).astype(np.float32))
                ]
            },
            {
                'name': 'å¤§æ•°æ®',
                'tensors': [
                    tf.constant(np.random.rand(10000, 128).astype(np.float32)),
                    tf.constant(np.random.rand(5000, 128).astype(np.float32)),
                    tf.constant(np.random.rand(15000, 128).astype(np.float32))
                ]
            }
        ]
        
        alignments = [32, 64, 128]
        
        for case in size_cases:
            for alignment in alignments:
                try:
                    tensors = case['tensors']
                    
                    # è®¡ç®—åŸå§‹å¤§å°
                    original_elements = sum(t.shape[0] for t in tensors)
                    
                    # åˆå¹¶
                    merged, offsets = tensor_concat_with_offsets_ops.tensor_concat_with_offsets(
                        tensors, alignment=alignment
                    )
                    
                    # è®¡ç®—å¯¹é½åå¤§å°
                    aligned_elements = merged.shape[0]
                    overhead = (aligned_elements - original_elements) / original_elements * 100
                    
                    # æ‹†åˆ†éªŒè¯
                    restored = tensor_split_by_offsets_ops.tensor_split_by_offsets(
                        merged, offsets
                    )
                    
                    # éªŒè¯æ­£ç¡®æ€§
                    all_correct = all(
                        np.allclose(orig.numpy(), rest.numpy(), rtol=1e-6)
                        for orig, rest in zip(tensors, restored)
                    )
                    
                    details = f"å†…å­˜å¼€é”€: {overhead:.1f}% (alignment={alignment})"
                    test_name = f"å†…å­˜ä½¿ç”¨_{case['name']}_align{alignment}"
                    self.log_result(test_name, all_correct, details)
                    
                except Exception as e:
                    test_name = f"å†…å­˜ä½¿ç”¨_{case['name']}_align{alignment}"
                    self.log_result(test_name, False, f"å¼‚å¸¸: {str(e)}")

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹TensorConcatWithOffsets + TensorSplitByOffsets è”åˆæµ‹è¯•")
        print("="*80)
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        self.test_basic_roundtrip()
        self.test_alignment_parameters()
        self.test_zero_copy_modes()
        self.test_performance_comparison()
        self.test_edge_cases()
        self.test_different_dtypes()
        self.test_memory_usage()
        
        # æ‰“å°æ€»ç»“
        self.print_summary()


def benchmark_detailed_performance():
    """è¯¦ç»†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\n" + "="*80)
    print("è¯¦ç»†æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("="*80)
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {
            'name': 'å°tensorå¤šåˆå¹¶',
            'tensors': [tf.constant(np.random.rand(100, 32).astype(np.float32)) for _ in range(50)],
            'alignment': 64
        },
        {
            'name': 'å¤§tensorå°‘åˆå¹¶', 
            'tensors': [tf.constant(np.random.rand(10000, 128).astype(np.float32)) for _ in range(5)],
            'alignment': 128
        },
        {
            'name': 'æ··åˆå¤§å°tensor',
            'tensors': [
                tf.constant(np.random.rand(100, 64).astype(np.float32)),
                tf.constant(np.random.rand(5000, 64).astype(np.float32)),
                tf.constant(np.random.rand(500, 64).astype(np.float32)),
                tf.constant(np.random.rand(10000, 64).astype(np.float32))
            ],
            'alignment': 64
        }
    ]
    
    for config in test_configs:
        print(f"\n--- {config['name']} ---")
        tensors = config['tensors']
        alignment = config['alignment']
        
        # åŸºå‡†æµ‹è¯•å‚æ•°
        warmup_iterations = 3
        test_iterations = 10
        
        # é¢„çƒ­
        for _ in range(warmup_iterations):
            merged, offsets = tensor_concat_with_offsets_ops.tensor_concat_with_offsets(
                tensors, alignment=alignment)
            _ = tensor_split_by_offsets_ops.tensor_split_by_offsets(merged, offsets)
        
        # è‡ªå®šä¹‰ç®—å­æµ‹è¯•
        custom_times = []
        for _ in range(test_iterations):
            start_time = time.time()
            
            merged, offsets = tensor_concat_with_offsets_ops.tensor_concat_with_offsets(
                tensors, alignment=alignment)
            restored = tensor_split_by_offsets_ops.tensor_split_by_offsets(
                merged, offsets, use_alignment=True)
            
            # å¼ºåˆ¶æ‰§è¡Œ
            _ = [t.numpy() for t in restored]
            
            custom_times.append(time.time() - start_time)
        
        # æ ‡å‡†TensorFlowç®—å­æµ‹è¯•  
        tf_times = []
        for _ in range(test_iterations):
            start_time = time.time()
            
            tf_merged = tf.concat(tensors, axis=0)
            lengths = [t.shape[0] for t in tensors]
            tf_restored = tf.split(tf_merged, lengths, axis=0)
            
            # å¼ºåˆ¶æ‰§è¡Œ
            _ = [t.numpy() for t in tf_restored]
            
            tf_times.append(time.time() - start_time)
        
        # ç»Ÿè®¡ç»“æœ
        custom_mean = np.mean(custom_times) * 1000
        custom_std = np.std(custom_times) * 1000
        tf_mean = np.mean(tf_times) * 1000
        tf_std = np.std(tf_times) * 1000
        
        print(f"tensoræ•°é‡: {len(tensors)}")
        print(f"æ€»å…ƒç´ æ•°: {sum(t.shape[0] for t in tensors):,}")
        print(f"è‡ªå®šä¹‰ç®—å­: {custom_mean:.2f} Â± {custom_std:.2f} ms")
        print(f"æ ‡å‡†ç®—å­:   {tf_mean:.2f} Â± {tf_std:.2f} ms")
        
        if custom_mean < tf_mean:
            speedup = tf_mean / custom_mean
            print(f"æ€§èƒ½æå‡: {speedup:.2f}x ğŸš€")
        else:
            slowdown = custom_mean / tf_mean
            print(f"æ€§èƒ½ä¸‹é™: {slowdown:.2f}x")
        
        # å†…å­˜å¼€é”€åˆ†æ
        merged_test, _ = tensor_concat_with_offsets_ops.tensor_concat_with_offsets(
            tensors, alignment=alignment)
        original_size = sum(t.shape[0] for t in tensors)
        aligned_size = merged_test.shape[0]
        overhead = (aligned_size - original_size) / original_size * 100
        print(f"å†…å­˜å¼€é”€: {overhead:.1f}%")


def main():
    """ä¸»å‡½æ•°"""
    print("TensorConcatWithOffsets + TensorSplitByOffsets è”åˆæµ‹è¯•å¥—ä»¶")
    print("Author: Custom Operators Team")
    print("Version: 1.0")
    print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
    print(f"è®¾å¤‡ä¿¡æ¯: {tf.config.list_physical_devices()}")
    
    # è¿è¡Œä¸»è¦æµ‹è¯•
    tester = ConcatSplitTester()
    tester.run_all_tests()
    
    # è¿è¡Œè¯¦ç»†æ€§èƒ½åŸºå‡†æµ‹è¯•
    benchmark_detailed_performance()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("1. å¯¹äºéœ€è¦é¢‘ç¹æ‹†åˆ†çš„åœºæ™¯ï¼Œæ¨èä½¿ç”¨alignment=64çš„å¯¹é½ä¼˜åŒ–")
    print("2. é›¶æ‹·è´æ¨¡å¼åœ¨æ»¡è¶³å¯¹é½æ¡ä»¶æ—¶å¯æ˜¾è‘—æå‡æ€§èƒ½")
    print("3. å†…å­˜å¼€é”€é€šå¸¸åœ¨5-20%ä¹‹é—´ï¼Œä½†æ‹†åˆ†æ€§èƒ½å¯æå‡50-200%")
    print("4. å¯¹äºå°tensoræˆ–å†…å­˜æ•æ„Ÿçš„åœºæ™¯ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨è¾ƒå°çš„alignmentå€¼")
    print("5. ä¸¤ä¸ªç®—å­çš„è”åˆä½¿ç”¨ä¸ºé«˜æ€§èƒ½tensoræ“ä½œæä¾›äº†å®Œæ•´çš„è§£å†³æ–¹æ¡ˆ")


if __name__ == "__main__":
    main()
